{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/lightgbm/__init__.py:46: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_9.4.1) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import lightgbm as lgb\n",
    "from scipy.sparse import vstack, hstack, csr_matrix, save_npz, load_npz, spmatrix\n",
    "from scipy.stats import binom\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer as CV\n",
    "import datetime\n",
    "import gc\n",
    "import re\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#文字列中の記号を除去する関数\n",
    "def arrange_words(text):\n",
    "    text = text.replace('!', '')\n",
    "    text = text.replace('?', '')\n",
    "    text = text.replace(',', '')\n",
    "    text = text.replace('.', '')\n",
    "    text = text.replace('“', '')\n",
    "    text = text.replace('”', '')\n",
    "    text = text.replace('‘', '')\n",
    "    text = text.replace('’', '')\n",
    "    text = text.replace('•', '')\n",
    "    text = text.replace('・', '')\n",
    "    text = text.replace('…', '')\n",
    "    text = text.replace(':', '')\n",
    "    text = text.replace(';', '')\n",
    "    text = text.replace('(', '')\n",
    "    text = text.replace(')', '')\n",
    "    text = text.replace('{', '')\n",
    "    text = text.replace('}', '')\n",
    "    text = text.replace('[', '')\n",
    "    text = text.replace(']', '')\n",
    "    text = text.replace('<', '')\n",
    "    text = text.replace('>', '')\n",
    "    text = text.replace('\\'', '')\n",
    "    text = text.replace('\\/', '')\n",
    "    text = text.replace('\"', '')\n",
    "    text = text.replace('-', ' ')\n",
    "    text = text.replace('_', ' ')\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = text.replace('\\r', ' ')\n",
    "    text = text.replace('#', '')\n",
    "    text = re.sub(r'[0-9]+', \"0\", text)\n",
    "    text = ' ' + text + ' '\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Series全体に含まれる全ての単語とその個数を集計する関数\n",
    "def get_word_counts(texts):\n",
    "    word_counts = defaultdict(int)\n",
    "    for text in texts.values:\n",
    "        for word in text.split(' '):\n",
    "            word_counts[word.lower()] += 1\n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#二項分布を用いて有効な単語を抽出する関数（今回は使用しない）\n",
    "def extract_useful_column(dataset, all_words, significance_level):\n",
    "    useful_words = []\n",
    "    p = dataset['target'].sum() / dataset['comment_text_arranged'].count()\n",
    "    dataset['target'] = dataset['target'].astype(np.int8)\n",
    "    texts = np.array(dataset)\n",
    "    counter = 1\n",
    "    for word in all_words:\n",
    "        agg = np.array([row[1] for row in texts if ' ' + word + ' ' in row[0]])\n",
    "        k = agg.sum()\n",
    "        N = len(agg)\n",
    "        p_value = binom.cdf(k, N, p)\n",
    "        if not((p_value>(significance_level)) and (p_value<(1-significance_level))):\n",
    "            print(word)\n",
    "            useful_words.append(word)\n",
    "    return useful_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#データ型を指定\n",
    "dtypes = {\n",
    "        'id':                                             'category',\n",
    "        'target':                                       'float16', \n",
    "        'comment_text':                           'category', \n",
    "        'severe_toxicity':                           'float16', \n",
    "        'obscene':                                    'float16', \n",
    "        'identity_attack':                           'float16', \n",
    "        'insult':                                         'float16', \n",
    "        'threat':                                        'float16', \n",
    "        'asian':                                         'float16', \n",
    "        'atheist':                                       'float16', \n",
    "        'bisexual':                                     'float16', \n",
    "        'black':                                         'float16', \n",
    "        'buddhist':                                    'float16', \n",
    "        'christian':                                    'float16', \n",
    "        'female':                                       'float16', \n",
    "        'heterosexual':                              'float16', \n",
    "        'hindu':                                         'float16', \n",
    "        'homosexual_gay_or_lesbian':        'float16', \n",
    "        'intellectual_or_learning_disability': 'float16', \n",
    "        'jewish':                                        'float16', \n",
    "        'latino':                                         'float16', \n",
    "        'male':                                          'float16', \n",
    "        'muslim':                                       'float16', \n",
    "        'other_disability':                           'float16', \n",
    "        'other_gender':                             'float16', \n",
    "        'other_race_or_ethnicity':              'float16', \n",
    "        'other_religion':                             'float16', \n",
    "        'other_sexual_orientation':             'float16', \n",
    "        'physical_disability':                       'float16', \n",
    "        'psychiatric_or_mental_illness':       'float16', \n",
    "        'transgender':                                'float16', \n",
    "        'white':                                          'float16', \n",
    "        'created_date':                              'category', \n",
    "        'publication_id':                             'category', \n",
    "        'parent_id':                                    'category', \n",
    "        'article_id':                                     'category', \n",
    "        'rating':                                         'category', \n",
    "        'funny':                                         'int8', \n",
    "        'wow':                                           'int8', \n",
    "        'sad':                                             'int8', \n",
    "        'likes':                                            'int8', \n",
    "        'disagree':                                     'int8', \n",
    "        'sexual_explicit':                             'float16', \n",
    "        'identity_annotator_count':             'int8', \n",
    "        'toxicity_annotator_count':             'int8', \n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5509"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#訓練データ・テストデータをロード\n",
    "train = pd.read_csv('train.csv', dtype=dtypes)\n",
    "test  = pd.read_csv('test.csv',  dtype=dtypes)\n",
    "train_ids = train.index\n",
    "test_ids  = test.index\n",
    "train_y = train['target'].apply(lambda x: 1 if x>=0.5 else 0)\n",
    "train_X = train.drop('target', axis=1)\n",
    "test_X = test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comment_textで使われている全ての単語の個数を取得\n",
    "train_X['comment_text_arranged'] = train_X['comment_text'].map(arrange_words)\n",
    "test_X['comment_text_arranged'] = test_X['comment_text'].map(arrange_words)\n",
    "train_word_counts = get_word_counts(train_X['comment_text_arranged'])\n",
    "test_word_counts = get_word_counts(test_X['comment_text_arranged'])\n",
    "train_word_counts_df = pd.DataFrame(list(train_word_counts.items()), columns=['word', 'count'])\n",
    "test_word_counts_df = pd.DataFrame(list(test_word_counts.items()), columns=['word', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#訓練データとテストデータ双方に存在する単語のみを抽出\n",
    "word_counts_df = train_word_counts_df.merge(test_word_counts_df, on='word', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#訓練データで極端に個数が少ない、又は訓練データ・テストデータで個数が極端に偏っている単語以外を抽出\n",
    "word_counts_df['scaled_total'] = word_counts_df['count_x'] + word_counts_df['count_y'] * 18\n",
    "word_counts_df = word_counts_df[word_counts_df['count_x']>10]\n",
    "word_counts_df = word_counts_df[(word_counts_df['count_x']/word_counts_df['scaled_total']>0.2) & (word_counts_df['count_x']/word_counts_df['scaled_total']<0.8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#それぞれの単語を特徴量とするデータセットを作成\n",
    "cv = CV(vocabulary=word_counts_df['word'].tolist())\n",
    "train_X_flattened = cv.fit_transform(list(train_X['comment_text_arranged'].values))\n",
    "test_X_flattened = cv.fit_transform(list(test_X['comment_text_arranged'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's auc: 0.939068\tvalid_0's binary_logloss: 0.157674\n",
      "[200]\tvalid_0's auc: 0.945462\tvalid_0's binary_logloss: 0.142701\n",
      "[300]\tvalid_0's auc: 0.947841\tvalid_0's binary_logloss: 0.138338\n",
      "[400]\tvalid_0's auc: 0.948532\tvalid_0's binary_logloss: 0.13681\n",
      "[500]\tvalid_0's auc: 0.948502\tvalid_0's binary_logloss: 0.136336\n",
      "Early stopping, best iteration is:\n",
      "[465]\tvalid_0's auc: 0.948602\tvalid_0's binary_logloss: 0.136408\n",
      "Fold 2\n",
      "\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's auc: 0.939062\tvalid_0's binary_logloss: 0.156635\n",
      "[200]\tvalid_0's auc: 0.944992\tvalid_0's binary_logloss: 0.142537\n",
      "[300]\tvalid_0's auc: 0.947221\tvalid_0's binary_logloss: 0.137916\n",
      "[400]\tvalid_0's auc: 0.947813\tvalid_0's binary_logloss: 0.136368\n",
      "[500]\tvalid_0's auc: 0.94759\tvalid_0's binary_logloss: 0.135927\n",
      "Early stopping, best iteration is:\n",
      "[409]\tvalid_0's auc: 0.947835\tvalid_0's binary_logloss: 0.136293\n",
      "Fold 3\n",
      "\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's auc: 0.940922\tvalid_0's binary_logloss: 0.155736\n",
      "[200]\tvalid_0's auc: 0.947174\tvalid_0's binary_logloss: 0.141273\n",
      "[300]\tvalid_0's auc: 0.949446\tvalid_0's binary_logloss: 0.13672\n",
      "[400]\tvalid_0's auc: 0.949888\tvalid_0's binary_logloss: 0.135104\n",
      "Early stopping, best iteration is:\n",
      "[396]\tvalid_0's auc: 0.94991\tvalid_0's binary_logloss: 0.135128\n",
      "Fold 4\n",
      "\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's auc: 0.940434\tvalid_0's binary_logloss: 0.156294\n",
      "[200]\tvalid_0's auc: 0.946627\tvalid_0's binary_logloss: 0.141238\n",
      "[300]\tvalid_0's auc: 0.948764\tvalid_0's binary_logloss: 0.136611\n",
      "[400]\tvalid_0's auc: 0.949173\tvalid_0's binary_logloss: 0.135001\n",
      "[500]\tvalid_0's auc: 0.948997\tvalid_0's binary_logloss: 0.134487\n",
      "Early stopping, best iteration is:\n",
      "[413]\tvalid_0's auc: 0.949191\tvalid_0's binary_logloss: 0.134855\n",
      "Fold 5\n",
      "\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's auc: 0.940171\tvalid_0's binary_logloss: 0.155797\n",
      "[200]\tvalid_0's auc: 0.94635\tvalid_0's binary_logloss: 0.141616\n",
      "[300]\tvalid_0's auc: 0.948541\tvalid_0's binary_logloss: 0.13729\n",
      "[400]\tvalid_0's auc: 0.949122\tvalid_0's binary_logloss: 0.135657\n",
      "Early stopping, best iteration is:\n",
      "[391]\tvalid_0's auc: 0.949132\tvalid_0's binary_logloss: 0.135735\n"
     ]
    }
   ],
   "source": [
    "#LightGBMで訓練し、予測値を作成。バリデーションにはStratifiedKFoldを用いる。\n",
    "lgb_test_result  = np.zeros(test_ids.shape[0])\n",
    "m = 100000\n",
    "counter = 0\n",
    "\n",
    "feature_importance_list = []\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "skf.get_n_splits(train_ids, np.array(train_y))\n",
    "\n",
    "for train_index, test_index in skf.split(train_ids, train_y):\n",
    "    \n",
    "    print('Fold {}\\n'.format(counter + 1))\n",
    "    X_fit = train_X_flattened[train_index]\n",
    "    X_val = train_X_flattened[test_index]\n",
    "    X_fit, X_val = csr_matrix(X_fit, dtype='float32'), csr_matrix(X_val, dtype='float32')\n",
    "    y_fit, y_val = train_y[train_index], train_y[test_index]\n",
    "    \n",
    "    gc.collect()\n",
    "\n",
    "    lgb_model = lgb.LGBMClassifier(max_depth=-1,\n",
    "                                   n_estimators=1000,\n",
    "                                   learning_rate=0.05,\n",
    "                                   num_leaves=2**9-1,\n",
    "                                   colsample_bytree=0.28,\n",
    "                                   objective='binary', \n",
    "                                   n_jobs=-1)\n",
    "                               \n",
    "    lgb_model.fit(X_fit, y_fit, eval_metric='auc', \n",
    "                  eval_set=[(X_val, y_val)], \n",
    "                  verbose=100, early_stopping_rounds=100)\n",
    "                  \n",
    "    del X_fit, X_val, y_fit, y_val, train_index, test_index\n",
    "    gc.collect()\n",
    "    \n",
    "    test = csr_matrix(test_X_flattened, dtype='float32')\n",
    "    lgb_test_result += lgb_model.predict_proba(test)[:,1]\n",
    "    counter += 1\n",
    "    \n",
    "    del test\n",
    "    gc.collect()\n",
    "    \n",
    "\n",
    "submission = pd.read_csv('sample_submission.csv')\n",
    "submission['prediction'] = lgb_test_result / counter\n",
    "submission.to_csv('lgb_submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
